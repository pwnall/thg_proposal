\section{Background and Related Work}
\label{sec:background}

Arguing about the security of an application running on an mainstream computer
using the x86 platform requires understanding the interactions between
all the parts of an x86 execution environment (protection modes, address
translation hardware, caches). This section provides an overview of these
features from the context of Intel TXT and SGX, touches on existing attacks on
TXT that took advantage of unexpected interactions between the features, and
points out holes in the SGX design that can be used to snoop on a program's
memory access patterns, which can reveal information about the private data
that the program is working on.


\subsection{Software Privilege Levels}
\label{sec:rings}

In an Infrastructure-as-a-Service (IaaS) cloud environment, such as Amazon EC2,
commodity CPUs run software at four different privilege levels: System
Management Mode (SMM), VMX root mode (also described as ring -1), kernel mode
(ring 0) and user mode (ring 3). This section outlines the consequences of a
successfully exploited vulnerability at each privilege level, and sets the
stage for analyzing the SGX protection model.

SMM is intended for use by the motherboard manufacturers to implement features
such as fan control and deep sleep, and/or to emulate missing hardware. SMM
mode is entered by asserting the SMI\# pin on the CPU, which was initially
designed exclusively for hardware use. However, the southbridges on most
motherboards provide a method for the hypervisor and kernel to get the SMI\#
pin asserted\footnote{Most southbridges assert SMI\# when a byte is written to
port 0xb2 via the \textsc{out} opcode}. The SMM code and data are stored in a
memory area called SMRAM, which, in theory, is not readable or writable when
the processor isn't running in SMM, but its protection mechanisms were bypassed
multiple times \cite{duflot2006smm} \cite{rutkowska2008remap}
\cite{wojtczuk2009smm}, and SMM-based rootkits have been demonstrated
\cite{wecherowski2009smm} \cite{wecherowski2009smm}.

IaaS cloud providers allow their customers to run their operating system of
choice in a virtualized environment. Hardware virtualization
\cite{uhlig2005intel}, called Virtual Machine Extensions (VMX) by Intel,
adds support for a \textit{hypervisor} (also called a Virtual Machine Monitor
/ VMM in the Intel documentation) that runs at a higher privilege level
(VMX root mode) than OS kernels, and is responsible for allocating hardware
resources across multiple operating systems that share the same physical
machine. Hypervisor code generally runs at ring 0 in VMX root mode.

The popular Xen hypervisor uses VMX root mode for improved peformance and a
smaller codebase \cite{zhang2008xen}. \cite{mccune2010trustvisor} proposes
using a hypervisor together with Intel TXT's dynamic root of trust for
measurement (DRTM) to implement trusted execution.
\cite{vasudevan2010requirements} argues that a dynamic root of trust mechanism,
like Intel TXT, is necessary to ensure a hypervisor's integrity. Unfortunately,
any SMM attack can be used to compromise Intel TXT \cite{wojtczuk2009txt}, and
the TXT implementation is complex enough that security vulnerabilities have
been found \cite{wojtczuk2009txt2} \cite{wojtczuk2011txt}.

Mainstream operating systems have monolithic kernels that include device
drivers, filesystem code, networking stacks, and video rendering functionality,
and run at ring 0 (kernel mode). Application code, such as a Web server or
a game client, runs at ring 3 (user mode). Software running inside an SGX
enclave (isolated environment) also executes at ring 3. The OS kernel, running
at ring 0, is responsible for scheduling the execution of code inside enclaves
and for allocating hardware resources (such as RAM) to the enclaves. In IaaS
cloud environments, the virtual machine images provided by customers run in VMX
non-root mode, so the kernel runs in VMX non-root ring 0, and the application
code runs in VMX non-root ring 3. SGX is compatible with VMX, and the
hypervisor is responsible for virtualizing a processor's SGX capabilities.

The monolithic kernel design leads to many opportunities for security
vulnerabilities in kernel code. For example the Linux kernel has had a
significant number of vulnerabilities patched every year, for the past 10 years
\cite{cvedetails2014linux} \cite{chen2011linux}. Also, a successful attack on
SMM or the hypervisor trivially translates into a compromised kernel. SGX
accounts for the possibility of a compromised kernel or hypervisor with an
attestation mechanism covering an enclave's contents, and with a hardware
isolation mechanism for the software inside the enclave. In \S \ref{sec:sgx},
we argue that SGX, as it is currently documented in \cite{intel2013sgxmanual},
appears to provide integrity for the software inside the enclave, but
definitely does not provide privacy, as a compromised SMM, hypervisor, or OS
kernel can obtain the enclave's memory access patterns at high granularity.


\subsection{Address Translation}
\label{sec:paging}

Recent commodity CPUs have many address translation modes, in order to run
legacy software dating back to 1990 natively. This section only addresses the
modes used in modern 64-bit operating systems and 64-bit cloud environments.

64-bit desktop operating systems use the addressing mode called IA-32e in
Intel programming manual \cite{intel2013manual}, which translates 48-bit
\textit{virtual addresses} into \textit{physical addresses} of at most 52
bits\footnote{The size of a physical address is CPU-dependent, and is 40 bits
for recent desktop CPUs and 44 bits for recent high-end server CPUs.}. The
bottom 12 bits of a virtual address are not changed by the translation. The top
36 bits are grouped into four 9-bit indexes used to navigate a data structure
called \textit{the page tables}. Despite its name, the data structure closely
resembles a perfectly balanced 512-ary search tree where the nodes do not store
keys. Each node is an array of 512 8-byte entries that contain the physical
addresses of the next-level children as well as some flags. The address of the
root node is stored in the CR3 register. The arrays in the last-level nodes
contain the physical addresses that are the result of the address translation.

Hardware virtualization (used in cloud computing) allows a hypervisor to run
multiple operating systems concurrently using the same physical memory, by
adding another layer of address translation. When extended page tables (EPT)
are enabled, the process above is used to translate from a virtual address into
a \textit{guest-physical address}. The translation from guest-physical
addresses to actual physical addresses uses the same process as above, except
the physical address of the root node is stored in a virtual machine control
structure.

Each entry in the page tables has some boolean flags, in addition to the
pointer to the next level. The following flags are particularly interesting for
our goals. The \textit{present} (P) flag is set to 0 for pages that have been
swapped out to disk. When address translation encounters a page table entry
where P is 0, the CPU generates a page fault (\#PF), and the OS kernel is
responsible for swapping the page back into RAM and resuming execution. If
an extended page table (EPT) entry has the P flag set 0, the CPU performs a VM
exit, and the hypervisor has an opportunity to bring the page into RAM. The
\textit{accessed} (A) flag is set whenever the address translation machinery
reads a page table entry, and the \textit{dirty} (D) flag is set when an entry
is accessed by a memory write operation. The A and D flags give the hypervisor
and OS kernel insight into application memory access patterns, to support page
eviciton algorithms.

Address translation applies to the software running in an SGX enclave
(isolated environment). This means that both the OS kernel and the hypervisor
have an impact over the mapping of virtual addresses to physical addresses, and
either of them can take advantage of page faults and the A and D flags in page
table entries to obtain the SGX application's memory access pattern, at page
granularity.


\subsection{Caching}
\label{sec:caching}

Desktop-class multi-core commodity CPUs have three levels of cache memory. Each
core has its own L1 and L2, and all cores share an L3 cache. All cache levels
are located on the CPU die so, according to our threat model, they can be
trusted. In particular, we can assume that software memory reads and writes for
locations that are already cached (and therefore do not generate DRAM requests)
cannot be directly observed by an attacker. This requires that the software's
memory is confired to be cacheable with a write-back policy, and our system
accounts for this requirement.

Cache accesses can be indirectly observed by various timing attacks
\cite{banescu2011cache}, by malicious software that time-shares a CPU or a core
with the targeted software and takes advantage of a specific cache's layout
and replacement algorithms. Our system prevents against these attacks, also by
taking advantage of some details of the cache implementation on commodity CPU
processors. This section describes the aspects that we rely on.
\cite{smith1982cache}, \cite{patterson2013architecture} and
\cite{hennessy2012architecture} all provide good backgrounds on low-level cache
implementation concepts.

To remain future-proof, the loader uses the CPUID instruction
\cite{intel2013manual} to query the CPU's cache layout and aborts when faced
with an implementation that doesn't match our design assumptions.

The \textit{cache line} is the atomic unit of data transfer between the caches
and the main memory. The cache line size is always a power of two. Assuming
$n$-bit memory addresses and a cache line size of $2^{l}$ bytes, the lowest
$l$ bits of a memory address are an offset into a cache line, and the highest
$n - l$ bits determine the cache line that is used to store the data at the
memory location. All recent processors have 64-byte cache lines.

The L1 and L2 caches in recent processors are multi-way set-associative with
direct set indexing. An $w$-way set-associative cache has its memory divided
into \textit{sets}, where each set has $w$ lines. A memory location can be
cached in any of the $w$ lines in a specific set that is determined by the
highest $n - l$ bits of the location's memory address. Direct set indexing
means that the $S$ sets in a cache are indexed from $0$ to $S - 1$, and a
memory location is cached in the set with index
$address_{n - 1 \ldots n - l} \bmod S$. In the common case where the number of
sets in a cache is a power of two, so $S = 2^{s}$, the lowest $l$ bits in an
address make up the cache line offset, the next $s$ bits are the set index.
The highest $n - s - l$ bits in an address are not used when selecting where a
memory location will be cached.

According to the CPUID instruction results and the Intel programming manuals
\cite{intel2013manual}, the L3 cache of recent processors does not use direct
set indexing, and instead uses a "complex" indexing scheme. The manuals do not
describe the scheme, so our system cannot rely on the L3 cache. Due to the lack
of a specification, our system treats L3 cache as untrusted memory. This is
rather unfortunate because, although the shared aspect of the L3 cache would
introduce additional complexity in a design, its large size (30MB on high-end
CPUs, compared to 256KB of per-core L2 cache) would make "taming" this last
level of cache very attractive.

When paging is enabled, translating the virtual memory addresses used by
software into physical memory addresses requires as many as 4 memory accesses,
for 64-bit software, or even 8 memory accesses in VMX non-root mode when
extended page tables (EPT) are in use. The CPU caches address translations in a
TLB (translation look-aside buffer). To make L1 caches as fast as possible, set
lookup does not depend on TLB lookup, so the set index in an L1 cache can only
use the address bits that are not impacted by address translation. Given a page
size $P = 2^{p}$ bytes, this means that $l + s \le p$. In the x86 architecture
$p = 12$, and all recent processors have 64-byte cache lines ($l = 6$) and
64 sets ($s = 6$).

The L2 cache in recent Intel processors is known to use physical indexing
\cite{patterson2013architecture}. The indexing method is not documented in
Intel's manuals and is not reported by the CPUID instruction, as it is
considered to be an implementation detail. Unfortunately, the indexing mehtod
decides the set index for a given memory location, which decides the cache
replacement policy, which leads to visible differences in execution times,
which enables timing attacks.

((address translation interactions wih cache))


\subsection{SGX Overview and Limitations}
\label{sec:sgx}

This section summarizes the aspects of the SGX documentation that are relevant
to our system. The interested but time-constrained reader is advised to read
\cite{mckeen2013innovative}, \cite{anati2013sgx}, and Chapters 1 (Introduction
to SGX), 3 (Enclave Operation), 4 (Enclave Exiting Events) and 6 (SGX
Interactions with IA32 and Intel 64 Architecture) of \cite{intel2013sgxmanual}
for a deeper understanding of SGX.

SGX introduces a new execution mode called \textit{enclave mode}. While in
enclave mode, the CPU can fetch code and data from an area of physical memory
that is isolated from software running outside enclave mode (including SMM) and
from the hardware connected to the system bus. The information stored in the
isolated memory is encrypted\footnote{Actually, the SGX manual carefully
tiptoes around this issue by saying "On implementations in which EPC is part of
system DRAM, the contents of the EPC are protected by an encryption engine."}
as it leaves the CPU. Furthermore, transitions to enclave mode must use
pre-specified entry points (like transitions from user mode to kernel mode),
and transitions from enclave mode store the processor state associated with
enclave code execution inside the isolated memory area.

SGX isolates the code and data in an enclave by placing it in a protected area
of physical memory called the Enclave Page Cache (EPC). EPC is a continuous
region of the Processor Reserved Memory (PRM) area, which in turn is a
continuous region of DRAM. Both the PRM and the EPC are set up by the BIOS
during the computer's boot sequence. The memory controller inside the CPU
blocks any DMA access to PRM, which protects the EPC area from hardware
connected to the system bus. The CPU disallows EPC access to software not
running in enclave mode.

SGX was designed to protect application-level code. Enclave execution always
happens in protected mode, at ring 3 (in user-mode, not kernel-mode). Also, SGX
supports applications whose memory footprint is larger than the EPC size. This
is accomplished by having the operating system map 4KB pages inside each
enclave's linear address space to EPC pages. The mapping is maintained in a
similar manner to how virtual memory works \cite{jacob1998virtual}, except that
EPC pages are paged out to RAM in an encrypted form, and SGX uses a data
structure to provide integrity and freshness guarantees for EPC pages that are
paged out and later paged in.

Logical memory addresses used by software in enclave mode are handled in a
similar way to ring 3 addresses. A limited form of segmentation is in effect,
meaning that logical addresses equal linear addresses. The normal paging rules
apply, meaning that linear addresses are translated into physical addresses
by walking a 3-level or 4-level data structure. SGX integrates with Intel's
Virtualization Technology \cite{uhlig2005intel} so, in non-root VMX mode with
Extended Page Tables (EPT) enabled, the result of walking the paging data
structures maintained by the OS is a guest-physical address, which is
translated into a physical address by walking another 3-level or 4-level
data structure maintained by the Virtual Machine Monitor (VMM).

When creating or paging in an EPC page, the operating system must declare the
linear address that will be used by enclave mode software to access the page.
On every address translation that results in the physical address of an EPC
page, the CPU ensures\footnote{A general protection (\#GP) fault occurs in case
of a linear address mismatch} that the linear address involved in the
translation matches the linear address associated with the EPC page. This
ensures that neither the OS nor the VMM can tamper with the enclave software's
view of memory.

Software running in enclave mode can cause page faults, under the same
conditions as any piece of software running at ring 3 would. Before the CPU
executes the normal steps for handling the fault, it performs an asynchronous
enclave exit (AEX), which saves the processor state in a designated area inside
the EPC, replaces all registers with bogus values, and resets the bottom 12
bits of register CR3, which contains the address of the page fault. SGX does
its best to hide the state of the execution inside enclave mode, but must
expose the page at which the fault occurred, so that the OS or the VMM know
which page needs to be paged in. This allows a curious OS or VMM to obtain a
page-level memory access trace for a program running inside an enclave, simply
by making sure that only one page is paged in at any given time.

There are many other channels that leak memory access information.

\begin{itemize}

\item Enclave mode sets the accessed and dirty bits appropriately on all levels
      of the paging structures involved in address translation, exposing access
      information to both the OS and the VMM.

\item Interrupts that cause an asynchronous exit (AEX) from enclave mode flush
      the processor's TLB, but leave the cache contents intact, so the OS or
      VMM can probe the cache to see which lines were replaced by the enclave
      software, which reveals information about the software's memory access
      patterns.

\item Returning into enclave mode from an AEX via the ERESUME instruction
      flushes the TLB, so the OS and VMM can get information about the enclave
      software's memory accesses even if the targeted memory is still cached,
      by marking the appropriate page table and/or page directory entries
      \textit{not present}.

\item Hyper-threading is compatible with SGX, so a malicious OS or VMM can
      schedule a thread on a logical processor on the same core as the logical
      processor running the enclave program. The two threads would share
      caches and execution units, so the second thread could use the
      processor's timestamp counter \cite{petters1999making} to obtain private
      information.

\item The PRM can be set up by the BIOS as write-back (WB) or uncacheable (UC)
      memory. If the PRM is set as uncacheable, an adversary with physical
      access and the tools to tap the system bus can see the enclave program's
      memory accesses, possibly at word granularity.

\end{itemize}

