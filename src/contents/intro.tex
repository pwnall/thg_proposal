\section{Introduction}
\label{sec:intro}

In order to achieve certain performance goals, software engineers must often
design systems where the owners of the data being processed are different from
the owners of the computing infrastructure used to process the data, who in
turn might be different from the suppliers of the software that processes the
data. This raises the problem that the data owner must trust both the owner
of the computing infrastructure and the software supplier to handle the data
according to some privacy policy.

\subsection{Motivation}
\label{sec:motivation}

The most popular instance of the above problem is cloud computing using public
clouds. For example, CheapoTax, a software provider, might provide tax return
preparation software as a service running on Amazon's public cloud. Users
upload their financial information, encrypted using TLS, to Amazon data
centers, where CheapoTax's computation uses the data to produce tax returns,
which are downloaded by the users in an encrypted form, using TLS. Currently,
the users must trust that neither Amazon nor CheapoTax will record their
private financial information. Users must also hope that neither Amazon's
infrastructure nor CheapoTax's software have bugs that would allow other rogue
actors to obtain the private financial data by running malicious software in
the same public Amazon cloud used by CheapoTax.

Another example of the problem we are aiming to solve is a multiplayer game of
any genre, such as League of Legends \cite{riot2009lol},
World of Warcraft \cite{blizzard2004wow}, StarCraft \cite{blizzard2010sc2}, and
Doom \cite{id2004doom}. The gameplay of many multiplayer games, such as the
ones listed above, rely on the fact that that players can only access a subset
of the game state that is dictated by the units they control. To compensate for
network latencies, many games entrust a player's game client with a superset
of the game state that the user should have access to. For example, in
StarCraft, the game client code running on the players' computers "knows" the
entire state, and is trusted to hide the information that the player should not
have access to \cite{hardy2009cheating}. The other games metioned above also
store information that the player shouldn't access in the game clients, leading
to opportunities for cheating \cite{youtube2013lolcheating}
\cite{youtube2008quake3cheating}.


\subsection{Threat Model}
\label{sec:threat}

Our threat model assumes that the infrastructure is shared by multiple ongoing
computations, potentially using software from different mutually distrusting
providers. The operating system and high-level software used to manage the
infrastructure may be malicious, and may cooperate with one or more malicious
pieces of software running on the infrastructure, including the software that
performs computations on the private data that must be protected. The
infrastructure owner may also mount a physical attack by attaching a device to
one of the computer's buses, such as the Front-Side Bus (FSB) used for DRAM
accesses or the PCI Express bus. We consider physical attacks to be
significantly more expensive to carry out than software attacks, and we treat
them separately where it makes sense to do so.

The threat model accurately represents a public cloud. Attackers can rent
resources and run malicious software, as well as run experiments to reveal and
exploit vulnerabilities in the infrastructure software. The programs that run
over the private data can also have vulnerabilities, which can be exploited by
attackers. An attacker can also motivate a datacenter maintainance employee to
attach a physical device to a server, but this attack is significantly harder
to mount than a purely software attack.

In the multiplayer game case, the private data is the full game state, and the
data owner is the same as the software supplier. The infrastructure is the
players' computers. Given that players have an incentive to cheat, it is
reasonable to assume that the infrastructure can behave maliciously. Our system
assumes that the players can exploit any vulnerability available in the game
software. Furthermore, players that desire an unfair advantage might install
software that is specifically designed for cheating. This software may be
running concurrently with the game software, and may include components that
run with OS privileges (e.g., Windows drivers or Linux / Darwin kernel
modules) or even SMM privileges (e.g., BIOS hacks). Last, players are willing
to attach hardware devices (known as \textit{mods} or \textit{modchips}) to
computers or game consoles to be able to cheat \cite{harris2007mod}, so
hardware attacks are even more relevant in the multiplayer game scenario.

Our threat model does not cover attacks that require intimiate knowledge of the
processor's hardware internals. Most importantly, we do not protect against
malicious OS-level software that can abuse a CPU's in-field microcode update
mechanism to create a vulnerability in the processor's security features, or
against attackers that have knowledge of existing vulnerabilities in the CPU.
We also do not protect against attacks that analyze the computer's power
consumption.

Last, the threat model does not include denial of service attacks. Our system
requires the operating system that manages the infrastructure to enable certain
CPU features. We do not trust the operating system, so our system checks for
the features' presence and enablement. If the checks fail, the system refuses
to decrypt private data and to perform any computation. This can lead to a
denial of service attack. At the same time, an attack that compromises the
operating system can cause a denial of service simply by disabling the network
driver, and that cannot be worked around as long as the OS is untrusted.

\subsection{Design Overview}
\label{sec:overview}

We plan to use the CPU's secure execution features to create an isolated
environment that the untrusted program will run in. We are most likely to end
up relying on Intel's Software Guard Extensions (SGX) \cite{anati2013sgx}.
Unfortunately, no processor with SGX has been released yet, so we must rely on
our interpretation of Intel's SGX programming reference manual
\cite{intel2013sgxmanual}.

SGX introduces a new execution mode called \textit{enclave mode}. While in
enclave mode, the CPU can fetch code and data from an area of physical memory
that is isolated from software running outside enclave mode (including SMM) and
from the hardware connected to the system bus. The information stored in the
isolated memory is encrypted\footnote{Actually, the SGX manual carefully
tiptoes around this issue by saying "On implementations in which EPC is part of
system DRAM, the contents of the EPC are protected by an encryption engine."}
as it leaves the CPU. Furthermore, transitions to enclave mode must use
pre-specified entry points (like transitions from user mode to kernel mode),
and transitions from enclave mode store the processor state associated with
enclave code execution inside the isolated memory area.

Based on the contents of Intel's manual, it seems that we can trust the SGX
implementation to guard the integrity of any computation that runs in enclave
mode against tampering attempts from other software on the platform. However,
on the privacy end, the provisions of SGX are lacking. The memory encryption
measures specified in the manual do not guard against other software learning
the memory access patterns of the software running inside enclave mode, which
can be used to gain knowledge about the private data that the computation is
running on. For example, it has been shown that cache timing attacks can be
used to recover encryption keys \cite{bonneau2006cache}
\cite{brumley2005remote} \cite{kocher1996timing}.

We avoid leaking memory access patterns by replacing all the instructions that
perform memory accesses in the software operating on private data with calls
to a memory manager that is part of our TCB. The memory manager implements an
oblivious RAM protocol \cite{stefanov2012path} to hide the software's memory
access patterns. We follow the approach of Google Native Client
\cite{yee2009native} \cite{sehr2010adapting}, namely we require that memory
accesses are replaced by calls to a function during the software's compilation
phase, and our TCB contains a loader that statically verifies the software to
ensure that it does not perform any direct memory access. This results in a
small TCB, compared to rewriting the software's machine code on the fly, and
we expect that a technique similar to RockSalt \cite{morrisett2012rocksalt} can
be used to prove the correctness of our verifier.

SGX provides an attestation mechanism that assures a remote user that a piece
of information was signed by a certain piece of software running inside a
enclave. The signature includes a cryptographic hash of the software running
inside the enclave. In our system, the signature only needs to cover the loader
and memory manager, as the loader verifies the program that computes on
private data, and ensures that the program meets the data owner's security
policy. The data owner must only trust our TCB, and the software provider can
iterate faster and rely on our verifier and memory manager to protect from
bugs that would result in private data leaks.


