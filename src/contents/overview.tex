\section{Design Overview}
\label{sec:overview}

We plan to use the CPU's secure execution features to create an isolated
environment that the untrusted program will run in. We are most likely to end
up relying Intel's Software Guard Extensions (SGX) \cite{anati2013sgx}.
Unfortunately, no processor with SGX has been released yet, so we must rely on
our interpretation of Intel's SGX programming reference manual
\cite{intel2013sgxmanual}.

SGX introduces a new execution mode called \textit{enclave mode}. While in
enclave mode, the CPU can fetch code and data from an area of physical memory
that is isolated from software running outside enclave mode (including SMM) and
from the hardware connected to the system bus. The information stored in the
isolated memory is encrypted\footnote{Actually, the SGX manual carefully
tiptoes around this issue by saying "On implementations in which EPC is part of
system DRAM, the contents of the EPC are protected by an encryption engine."}
as it leaves the CPU. Furthermore, transitions to enclave mode must use
pre-specified entry points (like transitions from user mode to kernel mode),
and transitions from enclave mode store the processor state associated with
enclave code execution inside the isolated memory area.

Based on the contents of Intel's manual, it seems that we can trust the SGX
implementation to guard the integrity of any computation that runs in enclave
mode against tampering attempts from other software on the platform. However,
on the privacy end, the provisions of SGX are lacking. The memory encryption
measures specified in the manual do not guard against other software learning
the memory access patterns of the software running inside enclave mode, which
can be used to gain knowledge about the private data that the computation is
running on. For example, it has been shown that cache timing attacks can be
used to recover encryption keys \cite{bonneau2006cache}
\cite{brumley2005remote} \cite{kocher1996timing}.

We avoid leaking memory access patterns by replacing all the instructions that
perform memory accesses in the software operating on private data with calls
to a memory manager that is part of our TCB. The memory manager implements an
oblivious RAM protocol \cite{stefanov2012path} to hide the software's memory
access patterns. We follow the approach of Google Native Client
\cite{yee2009native} \cite{sehr2010adapting}, namely we require that memory
accesses are replaced by calls to a function during the software's compilation
phase, and our TCB contains a loader that statically verifies the software to
ensure that it does not perform any direct memory access. This results in a
small TCB, compared to rewriting the software's machine code on the fly, and
we expect that a technique similar to RockSalt \cite{morrisett2012rocksalt} can
be used to prove the correctness of our verifier.

SGX provides an attestation mechanism that assures a remote user that a piece
of information was signed by a certain piece of software running inside a
enclave. The signature includes a cryptographic hash of the software running
inside the enclave. In our system, the signature only needs to cover the loader
and memory manager, as the loader verifies the program that computes on
private data, and ensures that the program meets the data owner's security
policy. The data owner must only trust our TCB, and the software provider can
iterate faster and rely on our verifier and memory manager to protect from
bugs that would result in private data leaks.


\subsection{Cachng and Paging Overview and Challenges}

Desktop-class multi-core commodity CPUs have three levels of cache memory. Each
core has its own L1 and L2, and all cores share an L3 cache. All cache levels
are located on the CPU die so, according to our threat model, they can be
trusted. In particular, we can assume that software memory reads and writes for
locations that are already cached (and therefore do not generate DRAM requests)
cannot be directly observed by an attacker. This requires that the software's
memory is confired to be cacheable with a write-back policy, and our system
accounts for this requirement.

Cache accesses can be indirectly observed by various timing attacks
\cite{banescu2011cache}, by malicious software that time-shares a CPU or a core
with the targeted software and takes advantage of a specific cache's layout
and replacement algorithms. Our system prevents against these attacks, also by
taking advantage of some details of the cache implementation on commodity CPU
processors. This section describes the aspects that we rely on.
\cite{smith1982cache}, \cite{patterson2013architecture} and
\cite{hennessy2012architecture} all provide good backgrounds on low-level cache
implementation concepts.

To remain future-proof, the loader uses the CPUID instruction
\cite{intel2013manual} to query the CPU's cache layout and aborts when faced
with an implementation that doesn't match our design assumptions.

The \textit{cache line} is the atomic unit of data transfer between the caches
and the main memory. The cache line size is always a power of two. Assuming
$n$-bit memory addresses and a cache line size of $2^{l}$ bytes, the lowest
$l$ bits of a memory address are an offset into a cache line, and the highest
$n - l$ bits determine the cache line that is used to store the data at the
memory location. All recent processors have 64-byte cache lines.

The L1 and L2 caches in recent processors are multi-way set-associative with
direct set indexing. An $w$-way set-associative cache has its memory divided
into \textit{sets}, where each set has $w$ lines. A memory location can be
cached in any of the $w$ lines in a specific set that is determined by the
highest $n - l$ bits of the location's memory address. Direct set indexing
means that the $S$ sets in a cache are indexed from $0$ to $S - 1$, and a
memory location is cached in the set with index
$address_{n - 1 \ldots n - l} \bmod S$. In the common case where the number of
sets in a cache is a power of two, so $S = 2^{s}$, the lowest $l$ bits in an
address make up the cache line offset, the next $s$ bits are the set index.
The highest $n - s - l$ bits in an address are not used when selecting where a
memory location will be cached.

According to the CPUID instruction results and the Intel programming manuals
\cite{intel2013manual}, the L3 cache of recent processors does not use direct
set indexing, and instead uses a "complex" indexing scheme. The manuals do not
describe the scheme, so our system cannot rely on the L3 cache. Due to the lack
of a specification, our system treats L3 cache as untrusted memory. This is
rather unfortunate because, although the shared aspect of the L3 cache would
introduce additional complexity in a design, its large size (30MB on high-end
CPUs, compared to 256KB of per-core L2 cache) would make "taming" this last
level of cache very attractive.

When paging is enabled, translating the virtual memory addresses used by
software into physical memory addresses requires as many as 4 memory accesses,
for 64-bit software, or even 8 memory accesses in VMX non-root mode when
extended page tables (EPT) are in use. The CPU caches address translations in a
TLB (translation look-aside buffer). To make L1 caches as fast as possible, set
lookup does not depend on TLB lookup, so the set index in an L1 cache can only
use the address bits that are not impacted by address translation. Given a page
size $P = 2^{p}$ bytes, this means that $l + s \le p$. In the x86 architecture
$p = 12$, and all recent processors have 64-byte cache lines ($l = 6$) and
64 sets ($s = 6$).

The L2 cache in recent Intel processors is known to use physical indexing
\cite{patterson2013architecture}. The indexing method is not documented in
Intel's manuals and is not reported by the CPUID instruction, as it is
considered to be an implementation detail. Unfortunately, the indexing mehtod
decides the set index for a given memory location, which decides the cache
replacement policy, which leads to visible differences in execution times,
which enables timing attacks.

((describe paging))

((address translation interactions wih cache))


\subsection{SGX Overview and Limitations}

This section summarizes the aspects of the SGX documentation that are relevant
to our system. The interested but time-constrained reader is advised to read
\cite{mckeen2013innovative}, \cite{anati2013sgx}, and Chapters 1 (Introduction
to SGX), 3 (Enclave Operation), 4 (Enclave Exiting Events) and 6 (SGX
Interactions with IA32 and Intel 64 Architecture) of \cite{intel2013sgxmanual}
for a deeper understanding of SGX.

SGX isolates the code and data in an enclave by placing it in a protected area
of physical memory called the Enclave Page Cache (EPC). EPC is a continuous
region of the Processor Reserved Memory (PRM) area, which in turn is a
continuous region of DRAM. Both the PRM and the EPC are set up by the BIOS
during the computer's boot sequence. The memory controller inside the CPU
blocks any DMA access to PRM, which protects the EPC area from hardware
connected to the system bus. The CPU disallows EPC access to software not
running in enclave mode.

SGX was designed to protect application-level code. Enclave execution always
happens in protected mode, at ring 3 (in user-mode, not kernel-mode). Also, SGX
supports applications whose memory footprint is larger than the EPC size. This
is accomplished by having the operating system map 4KB pages inside each
enclave's linear address space to EPC pages. The mapping is maintained in a
similar manner to how virtual memory works \cite{jacob1998virtual}, except that
EPC pages are paged out to RAM in an encrypted form, and SGX uses a data
structure to provide integrity and freshness guarantees for EPC pages that are
paged out and later paged in.

Logical memory addresses used by software in enclave mode are handled in a
similar way to ring 3 addresses. A limited form of segmentation is in effect,
meaning that logical addresses equal linear addresses. The normal paging rules
apply, meaning that linear addresses are translated into physical addresses
by walking a 3-level or 4-level data structure. SGX integrates with Intel's
Virtualization Technology \cite{uhlig2005intel} so, in non-root VMX mode with
Extended Page Tables (EPT) enabled, the result of walking the paging data
structures maintained by the OS is a guest-physical address, which is
translated into a physical address by walking another 3-level or 4-level
data structure maintained by the Virtual Machine Monitor (VMM).

When creating or paging in an EPC page, the operating system must declare the
linear address that will be used by enclave mode software to access the page.
On every address translation that results in the physical address of an EPC
page, the CPU ensures\footnote{A general protection (\#GP) fault occurs in case
of a linear address mismatch} that the linear address involved in the
translation matches the linear address associated with the EPC page. This
ensures that neither the OS nor the VMM can tamper with the enclave software's
view of memory.

Software running in enclave mode can cause page faults, under the same
conditions as any piece of software running at ring 3 would. Before the CPU
executes the normal steps for handling the fault, it performs an asynchronous
enclave exit (AEX), which saves the processor state in a designated area inside
the EPC, replaces all registers with bogus values, and resets the bottom 12
bits of register CR3, which contains the address of the page fault. SGX does
its best to hide the state of the execution inside enclave mode, but must
expose the page at which the fault occurred, so that the OS or the VMM know
which page needs to be paged in. This allows a curious OS or VMM to obtain a
page-level memory access trace for a program running inside an enclave, simply
by making sure that only one page is paged in at any given time.

There are many other channels that leak memory access information.

\begin{itemize}

\item Enclave mode sets the accessed and dirty bits appropriately on all levels
      of the paging structures involved in address translation, exposing access
      information to both the OS and the VMM.

\item Interrupts that cause an asynchronous exit (AEX) from enclave mode flush
      the processor's TLB, but leave the cache contents intact, so the OS or
      VMM can probe the cache to see which lines were replaced by the enclave
      software, which reveals information about the software's memory access
      patterns.

\item Returning into enclave mode from an AEX via the ERESUME instruction
      flushes the TLB, so the OS and VMM can get information about the enclave
      software's memory accesses even if the targeted memory is still cached,
      by marking the appropriate page table and/or page directory entries
      \textit{not present}.

\item Hyper-threading is compatible with SGX, so a malicious OS or VMM can
      schedule a thread on a logical processor on the same core as the logical
      processor running the enclave program. The two threads would share
      caches and execution units, so the second thread could use the
      processor's timestamp counter \cite{petters1999making} to obtain private
      information.

\item The PRM can be set up by the BIOS as write-back (WB) or uncacheable (UC)
      memory. If the PRM is set as uncacheable, an adversary with physical
      access and the tools to tap the system bus can see the enclave program's
      memory accesses, possibly at word granularity.

\end{itemize}


\subsection{Software Loading and Attestation}

